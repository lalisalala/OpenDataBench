# OpenDataBench

**OpenDataBench** is an open-source benchmark for evaluating **natural language dataset discovery** across open data portals and large language models (LLMs).  
It provides curated evaluation datasets, baseline results, evaluation scripts, and visualization tools to assess retrieval performance in multilingual, real-world settings.

---

## âœ¨ Key Features

- **Multilingual benchmark datasets**  
  - **London Datastore (LDS)** â€” English, municipal-scale.  
  - **GovData.de** â€” German, national-scale.  

## Baselines

The benchmark currently includes the following **baselines** for comparison:

- **Native portal search** â€” keyword-based search using the portalsâ€™ own CKAN interfaces:  
  - [London Datastore](https://data.london.gov.uk/) (English, municipal-scale)  
  - [GovData.de](https://www.govdata.de/) (German, national-scale)  

- **[data.europa.eu](https://data.europa.eu/)** â€” an aggregator portal that federates datasets from national and municipal portals, including LDS and GovData.  

- **ChatGPT-5.0** â€” instructed to return datasets specifically from the target portal.  

- **Gemini 2.5 Flash** â€” same setup as ChatGPT.  

- **DeepSeek 3.1** â€” search-enabled, restricted to the target portal by prompting.  

- **Ground truth annotations**  
  - 118 natural language queries.  
  - Fine-grained relevance scores (0â€“4).  

- **Evaluation metrics**  
  - Precision, Recall, F1.  
  - Average Relevance.  
  - nDCG@5.  
  - Hallucination Frequency & Rate (LLMs only).  

- **Reproducible evaluation pipeline**  
  - Scripts to compute baselines and generate plots.  
  - Outputs figures in `docs/figures/`.  

---

## ðŸ“‚ Repository Structure
```
OpenDataBench/
â”‚
â”œâ”€â”€ data/ # Evaluation datasets (GovData, LDS)
â”‚ â”œâ”€â”€ evaluation_dataset_GOV.json
â”‚ â”œâ”€â”€ evaluation_dataset_LDS.json
â”‚ â””â”€â”€ README.md
â”‚
â”œâ”€â”€ results/ # Raw system outputs (LLMs and portals)
â”‚ â”œâ”€â”€ GOV/
â”‚ â””â”€â”€ LDS/
â”‚
â”œâ”€â”€ baselines/ # Evaluated baseline JSONs
â”‚ â”œâ”€â”€ GOV/
â”‚ â””â”€â”€ LDS/
â”‚
â”œâ”€â”€ scripts/ # Evaluation and visualization scripts
â”‚ â”œâ”€â”€ compare_baseline.py
â”‚ â”œâ”€â”€ metrics_dashboard.py
â”‚ â””â”€â”€ README.md
â”‚
â”œâ”€â”€ docs/ # Documentation and figures
â”‚ â”œâ”€â”€ methodology.md
â”‚ â””â”€â”€ figures/
â”‚ â”œâ”€â”€ GOV/
â”‚ â””â”€â”€ LDS/
â”‚
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # Main repository overview
```

---

## ðŸš€ Quickstart

Clone the repo and install dependencies:

```bash
git clone https://github.com/your-username/OpenDataBench.git
cd OpenDataBench
pip install -r requirements.txt
```
Run evaluation for GovData.de:
```bash
cd scripts
python compare_baseline.py
python metrics_dashboard.py
```

Switch to London Datastore (LDS) by editing the DATASET variable in the scripts:
```bash
DATASET = "LDS"
```

Outputs:
```bash
Baseline JSONs â†’ baselines/{DATASET}/

Figures â†’ docs/figures/{DATASET}/
```
ðŸ“– Documentation

Methodology
 â€” detailed description of benchmark design, baselines, and metrics.

Scripts Guide
 â€” how to run evaluation and visualization scripts.

ðŸ“Š Example Figures

Evaluation metrics and hallucination comparisons are automatically generated in docs/figures/{DATASET}/.

ðŸ“œ License

This project is released under the MIT License.


## ðŸ”® Limitations & Future Work

- **Single-turn focus**:  
  The current benchmark only evaluates **first-turn queries** (single queries).  
  This allows us to standardize evaluation and ground truth, but it does not yet capture multi-turn interactions.  

- **Planned extensions**:  
  1. **Larger query set** â€” expanding the number and diversity of queries across domains.  
  2. **Multi-turn conversations** â€” extending the benchmark to support follow-up queries and conversational dataset discovery.  
