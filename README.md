# OpenDataBench

**OpenDataBench** is an open-source benchmark for evaluating **natural language dataset discovery** across open data portals and large language models (LLMs).  
It provides curated evaluation datasets, baseline results, evaluation scripts, and visualization tools to assess retrieval performance in multilingual, real-world settings.

---

## âœ¨ Key Features

**Multilingual benchmark datasets**  
  - **London Datastore (LDS)** â€” 68 English queries (municipal-scale).  
  - **GovData.de** â€” 50 German queries (national-scale).  
  - All queries come with **human-annotated ground truth** datasets and **graded relevance scores (0â€“4)**.  

## ğŸ“Š Benchmark Results

The benchmark currently covers both **London Datastore (LDS, English)** and **GovData.de (German)**.  
Below we show evaluation plots and tables summarizing baseline performance.

---

### ğŸ™ï¸ London Datastore (LDS)

**Evaluation Metrics (Precision, Recall, F1, Relevance, nDCG@5):**

![LDS Evaluation Metrics](docs/figures/LDS/evaluation_metrics_LDS.png)

**LLM Hallucination Metrics (Rate & Frequency):**

![LDS Hallucination Metrics](docs/figures/LDS/hallucination_metrics_LDS.png)

**Quantitative Results (averaged across queries):**

| System        | Precision | Recall | F1    | Avg. Relevance | nDCG  | Hallucination Rate | Hallucination Freq. |
|---------------|-----------|--------|-------|----------------|-------|---------------------|----------------------|
| DeepSeek      | 0.111â€“0.255 | 0.030â€“0.123 | 0.046â€“0.143 | 0.389â€“1.529 | 0.111â€“0.412 | **2.944** | **0.788** |
| Gemini        | 0.537â€“0.734 | 0.234â€“0.503 | 0.303â€“0.529 | 2.222â€“3.284 | 0.611â€“0.911 | 0.054 | 0.061 |
| ChatGPT       | 0.624â€“0.755 | 0.337â€“0.383 | 0.430â€“0.472 | 2.686â€“3.441 | 0.651â€“0.880 | 0.147 | 0.136 |
| LDS (portal)  | 0.267â€“0.529 | 0.248â€“0.375 | 0.212â€“0.408 | 2.000â€“3.030 | 0.531â€“0.795 | 0.000 | 0.000 |
| Europa        | 0.222â€“0.304 | 0.365â€“0.451 | 0.257â€“0.323 | 2.648â€“3.088 | 0.669â€“0.754 | 0.000 | 0.000 |

---

### ğŸ‡©ğŸ‡ª GovData.de (GOV)

**Evaluation Metrics (Precision, Recall, F1, Relevance, nDCG@5):**

![GovData Evaluation Metrics](docs/figures/GOV/evaluation_metrics_GOV.png)

**LLM Hallucination Metrics (Rate & Frequency):**

![GovData Hallucination Metrics](docs/figures/GOV/hallucination_metrics_GOV.png)

**Quantitative Results (averaged across queries):**

| System        | Precision | Recall | F1    | Avg. Relevance | nDCG  | Hallucination Rate | Hallucination Freq. |
|---------------|-----------|--------|-------|----------------|-------|---------------------|----------------------|
| DeepSeek      | 0.364â€“0.453 | 0.258â€“0.285 | 0.278â€“0.318 | 2.146â€“2.750 | 0.573â€“0.692 | 0.511 | 0.480 |
| Gemini        | 0.062â€“0.278 | 0.031â€“0.149 | 0.041â€“0.174 | 0.400â€“1.333 | 0.072â€“0.278 | **2.889** | **0.700** |
| ChatGPT       | 0.208â€“0.542 | 0.081â€“0.266 | 0.111â€“0.340 | 1.134â€“2.583 | 0.275â€“0.634 | 0.573 | 0.480 |
| GovData (portal) | 0.075â€“0.331 | 0.062â€“0.228 | 0.066â€“0.261 | 1.000â€“1.973 | 0.189â€“0.436 | 0.000 | 0.000 |
| Europa        | 0.123â€“0.219 | 0.139â€“0.219 | 0.128â€“0.211 | 1.150â€“2.667 | 0.263â€“0.543 | 0.000 | 0.000 |


## Baselines

The benchmark currently includes the following **baselines** for comparison:

- **Native portal search** â€” keyword-based search using the portalsâ€™ own CKAN interfaces:  
  - [London Datastore](https://data.london.gov.uk/) (English, municipal-scale)  
  - [GovData.de](https://www.govdata.de/) (German, national-scale)  

- **[data.europa.eu](https://data.europa.eu/)** â€” an aggregator portal that federates datasets from national and municipal portals, including LDS and GovData.  

- **ChatGPT-5.0** â€” instructed to return datasets specifically from the target portal.  

- **Gemini 2.5 Flash** â€” same setup as ChatGPT.  

- **DeepSeek 3.1** â€” search-enabled, restricted to the target portal by prompting.  

- **Ground truth annotations**  
  - 118 natural language queries.  
  - Fine-grained relevance scores (0â€“4).  

- **Evaluation metrics**  
  - Precision, Recall, F1.  
  - Average Relevance.  
  - nDCG@5.  
  - Hallucination Frequency & Rate (LLMs only).  

- **Reproducible evaluation pipeline**  
  - Scripts to compute baselines and generate plots.  
  - Outputs figures in `docs/figures/`.  

---

## ğŸ§¾ Query Styles

Following Walker et al. (2023) [Prompting Datasets: Data Discovery with Conversational Agents](https://arxiv.org/abs/2312.09947),  
we distinguish between three main ways that people express dataset needs:

| Query Style        | Example | Explanation |
|--------------------|---------|-------------|
| **Dataset Request** | â€œCan you find me the *German credit fairness dataset*?â€ | The dataset is explicitly named or requested directly. |
| **Described Dataset** | â€œProvide me with datasets about *Islamophobia in Italian Tweets* labelled as hate speech.â€ | The dataset is not named, but its properties/content are described. |
| **Implied Dataset** | â€œWhat was the *average property price in London from 2000â€“2010*?â€ | The request implies that a dataset is needed to answer, without naming it as a dataset. |

These three modes form the basis of our benchmark evaluation.

## ğŸ“‚ Repository Structure
```
OpenDataBench/
â”‚
â”œâ”€â”€ data/ # Evaluation datasets (GovData, LDS)
â”‚ â”œâ”€â”€ evaluation_dataset_GOV.json
â”‚ â”œâ”€â”€ evaluation_dataset_LDS.json
â”‚ â””â”€â”€ README.md
â”‚
â”œâ”€â”€ results/ # Raw system outputs (LLMs and portals)
â”‚ â”œâ”€â”€ GOV/
â”‚ â””â”€â”€ LDS/
â”‚
â”œâ”€â”€ baselines/ # Evaluated baseline JSONs
â”‚ â”œâ”€â”€ GOV/
â”‚ â””â”€â”€ LDS/
â”‚
â”œâ”€â”€ scripts/ # Evaluation and visualization scripts
â”‚ â”œâ”€â”€ compare_baseline.py
â”‚ â”œâ”€â”€ metrics_dashboard.py
â”‚ â””â”€â”€ README.md
â”‚
â”œâ”€â”€ docs/ # Documentation and figures
â”‚ â”œâ”€â”€ methodology.md
â”‚ â””â”€â”€ figures/
â”‚ â”œâ”€â”€ GOV/
â”‚ â””â”€â”€ LDS/
â”‚
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # Main repository overview
```

---

## ğŸš€ Quickstart

Clone the repo and install dependencies:

```bash
git clone https://github.com/your-username/OpenDataBench.git
cd OpenDataBench
pip install -r requirements.txt
```
Run evaluation for GovData.de:
```bash
cd scripts
python compare_baseline.py
python metrics_dashboard.py
```

Switch to London Datastore (LDS) by editing the DATASET variable in the scripts:
```bash
DATASET = "LDS"
```

Outputs:
```bash
Baseline JSONs â†’ baselines/{DATASET}/

Figures â†’ docs/figures/{DATASET}/
```
ğŸ“– Documentation

Methodology
 â€” detailed description of benchmark design, baselines, and metrics.

Scripts Guide
 â€” how to run evaluation and visualization scripts.

ğŸ“Š Example Figures

Evaluation metrics and hallucination comparisons are automatically generated in docs/figures/{DATASET}/.

ğŸ“œ License

This project is released under the MIT License.


## ğŸ”® Limitations & Future Work

- **Single-turn focus**:  
  The current benchmark only evaluates **first-turn queries** (single queries).  
  This allows us to standardize evaluation and ground truth, but it does not yet capture multi-turn interactions.  

- **Planned extensions**:  
  1. **Larger query set** â€” expanding the number and diversity of queries across domains.  
  2. **Multi-turn conversations** â€” extending the benchmark to support follow-up queries and conversational dataset discovery.  
