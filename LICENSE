# OpenDataBench

**OpenDataBench** is an open-source benchmark for evaluating **natural language dataset discovery** across open data portals and large language models (LLMs).  
It provides curated evaluation datasets, baseline results, evaluation scripts, and visualization tools to assess retrieval performance in multilingual, real-world settings.

---

## âœ¨ Key Features

- **Multilingual benchmark datasets**  
  - **London Datastore (LDS)** â€” English, municipal-scale.  
  - **GovData.de** â€” German, national-scale.  

- **Baselines**  
  - Native portal keyword search.  
  - Aggregated search on [data.europa.eu](https://data.europa.eu/).  
  - LLM-based retrieval (ChatGPT, Gemini, DeepSeek).  
  - Our proposed system **OpenDORA** (RAG-based).  

- **Ground truth annotations**  
  - 118 natural language queries.  
  - Fine-grained relevance scores (0â€“4).  

- **Evaluation metrics**  
  - Precision, Recall, F1.  
  - Average Relevance.  
  - nDCG@5.  
  - Hallucination Frequency & Rate (LLMs only).  

- **Reproducible evaluation pipeline**  
  - Scripts to compute baselines and generate plots.  
  - Outputs figures in `docs/figures/`.  

---

## ðŸ“‚ Repository Structure

OpenDataBench/
â”‚
â”œâ”€â”€ data/ # Evaluation datasets (GovData, LDS)
â”‚ â”œâ”€â”€ evaluation_dataset_GOV.json
â”‚ â”œâ”€â”€ evaluation_dataset_LDS.json
â”‚
â”œâ”€â”€ results/ # Raw system outputs (LLMs and portals)
â”‚ â”œâ”€â”€ GOV/
â”‚ â””â”€â”€ LDS/
â”‚
â”œâ”€â”€ baselines/ # Evaluated baseline JSONs
â”‚ â”œâ”€â”€ GOV/
â”‚ â””â”€â”€ LDS/
â”‚
â”œâ”€â”€ scripts/ # Evaluation and visualization scripts
â”‚ â”œâ”€â”€ compare_baseline.py
â”‚ â”œâ”€â”€ metrics_dashboard.py
â”‚ â””â”€â”€ README.md
â”‚
â”œâ”€â”€ docs/ # Documentation and figures
â”‚ â”œâ”€â”€ methodology.md
â”‚ â””â”€â”€ figures/
â”‚ â”œâ”€â”€ GOV/
â”‚ â””â”€â”€ LDS/
â”‚
â””â”€â”€ README.md # (this file)


---

## ðŸš€ Quickstart

Clone the repo and install dependencies:

```bash
git clone https://github.com/your-username/OpenDataBench.git
cd OpenDataBench
pip install -r requirements.txt

Run evaluation for GovData.de:

cd scripts
python compare_baseline.py
python metrics_dashboard.py


Switch to London Datastore (LDS) by editing the DATASET variable in the scripts:

DATASET = "LDS"


Outputs:

Baseline JSONs â†’ baselines/{DATASET}/

Figures â†’ docs/figures/{DATASET}/

ðŸ“– Documentation

Methodology
 â€” detailed description of benchmark design, baselines, and metrics.

Scripts Guide
 â€” how to run evaluation and visualization scripts.

ðŸ“Š Example Figures

Evaluation metrics and hallucination comparisons are automatically generated in docs/figures/{DATASET}/.

ðŸ“œ License

This project is released under the MIT License.